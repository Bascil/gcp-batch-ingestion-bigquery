steps:
  # 1. Fetch the source code
  # - name: gcr.io/cloud-builders/git
  #   args: ['clone', 'https://github.com/Bascil/gcp-batch-ingestion-bigquery']

  # 1a. Set up GCS & BQ etc. using public terraform Docker image
  - name: hashicorp/terraform
    args: ['init', '-reconfigure']
    dir: 'terraform'

  # 1b. Create the GCS bucket using Terraform
  - name: hashicorp/terraform
    id: terraform-apply
    args: ['apply', '-auto-approve']
    dir: 'terraform'

  # 2. Build and run the Dataflow pipeline (staged template)
  - name: gcr.io/cloud-builders/gradle
    args: ['build', 'run']
    waitFor: ['terraform-apply']

  # 3a. Install npm & run tests
  - name: gcr.io/cloud-builders/npm
    id: npm-install-test
    args: ['install-test']
    dir: 'cloud-function'
    waitFor: ['terraform-apply']

  # 3b. Deploy the Cloud Function that listens to the bucket
  - name: gcr.io/cloud-builders/gcloud
    id: function-deploy
    args:
      [
        'functions',
        'deploy',
        '--runtime',
        'nodejs8',
        'goWithTheDataFlow',
        '--stage-bucket=gs://artisto-batch-pipeline',
        '--trigger-bucket=gs://artisto-batch-pipeline',
      ]
    dir: 'cloud-function'
    waitFor: ['npm-install-test']

  # 4. Trigger the pipeline for demo purposes
  - name: gcr.io/cloud-builders/gsutil
    args:
      [
        'cp',
        'gs://test-file-for-dataflow/*',
        'gs://artisto-batch-pipeline/upload/file.csv',
      ]

# 5. Copy tarball/archive to GCS for more shenanigans later
artifacts:
  objects:
    location: 'gs://artisto-batch-pipeline/artifacts'
    paths: ['build/distributions/*.*']
